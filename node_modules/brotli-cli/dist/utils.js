"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.writeFile = exports.readFile = void 0;
const fs_1 = __importDefault(require("fs"));
const util_1 = __importDefault(require("util"));
const buffer_1 = __importDefault(require("buffer"));
const os_1 = __importDefault(require("os"));
const stat = util_1.default.promisify(fs_1.default.stat);
const is64Bit = ["x64", "arm64", "ppc64", "s390x"].includes(os_1.default.arch());
const readFileSizeLimit = is64Bit ? Math.pow(2, 31) - 1 : Math.pow(2, 30) - 1;
const nativeReadFile = util_1.default.promisify(fs_1.default.readFile);
const readFile = async (path) => {
    const fileSize = await stat(path).then(stats => stats.size);
    if (fileSize > buffer_1.default.constants.MAX_LENGTH) {
        throw new Error(`File ${path} is too big to process, `
            + `${fileSize} bytes read but max ${buffer_1.default.constants.MAX_LENGTH} bytes allowed`);
    }
    if (fileSize < readFileSizeLimit) {
        return nativeReadFile(path);
    }
    return new Promise((resolve, reject) => {
        const stream = fs_1.default.createReadStream(path);
        const chunks = [];
        stream.on("data", chunk => {
            chunks.push(Buffer.from(chunk));
        });
        stream.on("end", () => {
            resolve(Buffer.concat(chunks));
        });
        stream.on("error", reject);
    });
};
exports.readFile = readFile;
const writeFile = util_1.default.promisify(fs_1.default.writeFile);
exports.writeFile = writeFile;
//# sourceMappingURL=utils.js.map